{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.20.3)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.6.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.18.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.5.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "# load existing data \n",
    "try:\n",
    "  df = pd.read_csv(\"./database/disease_to_acu_and_symptom_modify_ZH_faceonly_embedding.csv\")\n",
    "  df['symptoms'] = df['symptoms'].apply(lambda x: ast.literal_eval(x))\n",
    "  df['acupoints'] = df['acupoints'].apply(lambda x: ast.literal_eval(x))\n",
    "  df['symptoms_embeddings'] = df['symptoms_embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "  print(type(df.loc[0,'symptoms']), type(df.loc[0,'acupoints']),type(df.loc[0,'symptoms_embeddings']),type(df.loc[0,'symptoms_embeddings'][0]))\n",
    "  print(df.head(5))\n",
    "  embedder = SentenceTransformer('bert-base-chinese')\n",
    "except:\n",
    "  print('no existing data ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create symptoms _embeddings\n",
    "embedder = SentenceTransformer('bert-base-chinese')\n",
    "\n",
    "df['symptoms_embeddings'] = [[None]] * len(df)\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "  df.loc[index,'symptoms_embeddings'] = embedder.encode(df.loc[index,'symptoms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caculate embedding and save to csV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "# change np array to list (for saving)\n",
    "for index, row in df1.iterrows():\n",
    "  df1.loc[index,'symptoms_embeddings'] = df1.loc[index,'symptoms_embeddings'].tolist()\n",
    "df1.to_csv('./database/disease_to_acu_and_symptom_modify_ZH_faceonly_embedding.csv',index=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read embedding file and predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\tsoudibi/.cache\\torch\\sentence_transformers\\bert-base-chinese. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\tsoudibi/.cache\\torch\\sentence_transformers\\bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# to use the service, server need to 'pip install -U sentence-transformers '\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def read_embedding_CSV(file_name = \"disease_to_acu_and_symptom_modify_ZH_faceonly_embedding.csv\"):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['symptoms'] = df['symptoms'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['acupoints'] = df['acupoints'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['symptoms_embeddings'] = df['symptoms_embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "    # print(type(df.loc[0,'symptoms']), type(df.loc[0,'acupoints']),type(df.loc[0,'symptoms_embeddings']),type(df.loc[0,'symptoms_embeddings'][0]))\n",
    "    # print(df.head(5))\n",
    "    return df\n",
    "\n",
    "def load_embedder():\n",
    "    embedder = SentenceTransformer('bert-base-chinese')\n",
    "    return embedder\n",
    "\n",
    "df = read_embedding_CSV()\n",
    "embedder = load_embedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Query: ['眼睛癢', '眼窩痛']\n",
      "眼眶脹痛 (Score: 1.7586)\n",
      "眶上神經痛 (Score: 1.7412)\n",
      "眼內或周圍疼痛 (Score: 1.7233)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def query_disease(queries):\n",
    "    query_embeddings = embedder.encode(queries)\n",
    "    threshold = 0.80\n",
    "    \n",
    "    # create empty disease dictionary\n",
    "    score_dist = {} \n",
    "    for index, row in df.iterrows():\n",
    "        score_dist[row['disease_name']] = 0 # use dictionary to store disease name and score\n",
    "\n",
    "    for query, query_embedding in zip(queries, query_embeddings):\n",
    "        for index, row in df.iterrows():\n",
    "            dists = scipy.spatial.distance.cdist([query_embedding], df.loc[index,'symptoms_embeddings'], \"cosine\")[0]\n",
    "\n",
    "            final_dist = 0\n",
    "            avaliable_count = 0\n",
    "            for dist in dists:\n",
    "                if 1-dist >= threshold:\n",
    "                    final_dist += dist\n",
    "                    avaliable_count += 1\n",
    "            # take mean value of all diatence as score_dist\n",
    "            try:\n",
    "                score_dist[row['disease_name']] = score_dist[row['disease_name']] + (1-(final_dist / avaliable_count))\n",
    "                # score_dist.append(final_dist / avaliable_count)\n",
    "            except:\n",
    "                score_dist[row['disease_name']] = score_dist[row['disease_name']] + 0\n",
    "\n",
    "        # sort by distence\n",
    "        results = sorted(score_dist.items(), key=lambda x:x[1], reverse=True)\n",
    "    print(\"======================\")\n",
    "    print(\"Query:\", queries)\n",
    "    for name, distance in results[0:3]:\n",
    "        print(name, \"(Score: %.4f)\" % (distance))\n",
    "    \n",
    "    # returning top 3 high result\n",
    "    return results[0:3]\n",
    "\n",
    "queries = ['眼睛癢','眼窩痛',]\n",
    "query_disease(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the service, server need to 'pip install -U sentence-transformers '\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Medical_Bot():\n",
    "        \n",
    "    def read_embedding_CSV(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.DB_file_name)\n",
    "            df['symptoms'] = df['symptoms'].apply(lambda x: ast.literal_eval(x))\n",
    "            df['acupoints'] = df['acupoints'].apply(lambda x: ast.literal_eval(x))\n",
    "            df['symptoms_embeddings'] = df['symptoms_embeddings'].apply(lambda x: ast.literal_eval(x))\n",
    "            # print(type(df.loc[0,'symptoms']), type(df.loc[0,'acupoints']),type(df.loc[0,'symptoms_embeddings']),type(df.loc[0,'symptoms_embeddings'][0]))\n",
    "            # print(df.head(5))\n",
    "            self.DataBase = df\n",
    "            return \n",
    "        except:\n",
    "            print('error when read_embedding_CSV')\n",
    "\n",
    "            \n",
    "    def query_disease(self, queries):\n",
    "        # encode quries\n",
    "        query_embeddings = embedder.encode(queries)\n",
    "        \n",
    "        # create empty disease dictionary\n",
    "        score_dist = {} \n",
    "        for index, row in df.iterrows():\n",
    "            score_dist[row['disease_name']] = 0 # use dictionary to store disease name and score\n",
    "\n",
    "        for query, query_embedding in zip(queries, query_embeddings):\n",
    "            for index, row in df.iterrows():\n",
    "                dists = scipy.spatial.distance.cdist([query_embedding], df.loc[index,'symptoms_embeddings'], \"cosine\")[0]\n",
    "\n",
    "                final_dist = 0\n",
    "                avaliable_count = 0\n",
    "                for dist in dists:\n",
    "                    if 1-dist >= self.query_threshold :\n",
    "                        final_dist += dist\n",
    "                        avaliable_count += 1\n",
    "                # take mean value of all diatence as score_dist\n",
    "                try:\n",
    "                    score_dist[row['disease_name']] = score_dist[row['disease_name']] + (1-(final_dist / avaliable_count))\n",
    "                    # score_dist.append(final_dist / avaliable_count)\n",
    "                except:\n",
    "                    score_dist[row['disease_name']] = score_dist[row['disease_name']] + 0\n",
    "\n",
    "            # sort by distence\n",
    "            results = sorted(score_dist.items(), key=lambda x:x[1], reverse=True)\n",
    "        print(\"======================\")\n",
    "        print(\"Query:\", queries)\n",
    "        for name, distance in results[0:3]:\n",
    "            print(name, \"(Score: %.4f)\" % (distance))\n",
    "        \n",
    "        # returning top 3 high result\n",
    "        return results[0:self.query_return_number]\n",
    "            \n",
    "    def __init__(self):        \n",
    "        # DB settings\n",
    "        print('reading database file...')\n",
    "        self.DB_file_name = \"disease_to_acu_and_symptom_modify_ZH_faceonly_embedding.csv\"\n",
    "        self.read_embedding_CSV()\n",
    "        print('done!')\n",
    "        \n",
    "        # load embedder\n",
    "        print('loading embedder...')\n",
    "        self.embedder = SentenceTransformer('bert-base-chinese')\n",
    "        print('done!')\n",
    "        \n",
    "        # query settings\n",
    "        self.query_return_number = 3\n",
    "        self.query_threshold = 0.80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading database file...\n",
      "done!\n",
      "loading embedder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\tsoudibi/.cache\\torch\\sentence_transformers\\bert-base-chinese. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\tsoudibi/.cache\\torch\\sentence_transformers\\bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "MB = Medical_Bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Query: ['眼睛癢', '眼窩痛']\n",
      "眼眶脹痛 (Score: 1.7586)\n",
      "眶上神經痛 (Score: 1.7412)\n",
      "眼內或周圍疼痛 (Score: 1.7233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('眼眶脹痛', 1.758614956663484),\n",
       " ('眶上神經痛', 1.7412440078948541),\n",
       " ('眼內或周圍疼痛', 1.7232594403991994)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB.query_disease(['眼睛癢','眼窩痛',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正反慈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13368/3266224282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cosine\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2930\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2931\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XA must be a 2-dimensional array.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2932\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msB\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XB must be a 2-dimensional array.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "AA = '會'\n",
    "BB = '有'\n",
    "A = embedder.encode(AA)\n",
    "B = embedder.encode(BB)\n",
    "dists = scipy.spatial.distance.cdist([A], [B], \"cosine\")[0]\n",
    "print(AA,' ',BB,' :', dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
